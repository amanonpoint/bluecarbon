{
  "chunks": [
    {
      "id": 463093387711036270,
      "distance": 1.1362597942352295,
      "text": "#### Who Should Read This Book?  \nThis book is intended for anyone who is interested in using modern statistical methods for modeling and prediction from data. This group includes scientists, engineers, data analysts, or *quants*, but also less technical individuals with degrees in non-quantitative fields such as the social sciences or business. We expect that the reader will have had at least one elementary course in statistics. Background in linear regression is also useful, though not required, since we review the key concepts behind linear regression in Chapter 3. The mathematical level of this book is modest, and a detailed knowledge of matrix operations is not required. This book provides an introduction to the statistical programming language R. Previous exposure to a programming language, such as MATLAB or Python, is useful but not required.  \nWe have successfully taught material at this level to master's and PhD students in business, computer science, biology, earth sciences, psychology, and many other areas of the physical and social sciences. This book could also be appropriate for advanced undergraduates who have already taken a course on linear regression. In the context of a more mathematically rigorous course in which ESL serves as the primary textbook, ISL could be used as a supplementary text for teaching computational aspects of the various approaches.",
      "metadata": {
        "header": "Introduction",
        "subheader": "Who Should Read This Book?",
        "page": "20",
        "chunk_id": "chk_e0e7eec5-f6a9-4129-86d4-6cefe1cab7ed",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 216
      }
    },
    {
      "id": 463093387711036265,
      "distance": 1.1700067520141602,
      "text": "#### Stock Market Data  \nThe Wage data involves predicting a *continuous* or *quantitative* output value. This is often referred to as a *regression* problem. However, in certain cases we may instead wish to predict a non-numerical value—that is, a *categorical*  \n{14}------------------------------------------------  \n![](_page_14_Figure_2.jpeg)  \n**FIGURE 1.2.** Left: Boxplots of the previous day's percentage change in the S&P index for the days for which the market increased or decreased, obtained from the Smarket data. Center and Right: Same as left panel, but the percentage changes for 2 and 3 days previous are shown.  \nor *qualitative* output. For example, in Chapter 4 we examine a stock market data set that contains the daily movements in the Standard & Poor's 500 (S&P) stock index over a 5-year period between 2001 and 2005. We refer to this as the Smarket data. The goal is to predict whether the index will *increase* or *decrease* on a given day using the past 5 days' percentage changes in the index. Here the statistical learning problem does not involve predicting a numerical value. Instead it involves predicting whether a given day's stock market performance will fall into the Up bucket or the Down bucket. This is known as a *classification* problem. A model that could accurately predict the direction in which the market will move would be very useful!  \nThe left-hand panel of Figure 1.2 displays two boxplots of the previous day's percentage changes in the stock index: one for the 648 days for which the market increased on the subsequent day, and one for the 602 days for which the market decreased. The two plots look almost identical, suggesting that there is no simple strategy for using yesterday's movement in the S&P to predict today's returns. The remaining panels, which display boxplots for the percentage changes 2 and 3 days previous to today, similarly indicate little association between past and present returns. Of course, this lack of pattern is to be expected: in the presence of strong correlations between successive days' returns, one could adopt a simple trading strategy to generate profits from the market. Nevertheless, in Chapter 4, we explore these data using several different statistical learning methods. Interestingly, there are hints of some weak trends in the data that suggest that, at least for this 5-year period, it is possible to correctly predict the direction of movement in the market approximately 60% of the time (Figure 1.3).  \n{15}------------------------------------------------",
      "metadata": {
        "header": "Introduction",
        "subheader": "Stock Market Data",
        "page": "13-15",
        "chunk_id": "chk_2ae8a133-db4f-4e8d-8225-bb28ee136004",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 401
      }
    },
    {
      "id": 463093387711036257,
      "distance": 1.1984515190124512,
      "text": "# An Introduction to Statistical Learning  \nwith Applications in R  \n![](_page_0_Picture_3.jpeg)  \n{1}------------------------------------------------  \nGareth James Department of Information and Operations Management University of Southern California Los Angeles, CA, USA  \nTrevor Hastie Department of Statistics Stanford University Stanford, CA, USA  \nDaniela Witten Department of Biostatistics University of Washington Seattle, WA, USA  \nRobert Tibshirani Department of Statistics Stanford University Stanford, CA, USA  \nISSN 1431-875X ISBN 978-1-4614-7137-0 ISBN 978-1-4614-7138-7 (eBook) DOI 10.1007/978-1-4614-7138-7 Springer New York Heidelberg Dordrecht London  \nLibrary of Congress Control Number: 2013936251  \n© Springer Science+Business Media New York 2013 (Corrected at 6th printing 2015)  \nThis work is subject to copyright. All rights are reserved by the Publisher, whether the whole or part of the material is concerned, specifically the rights of translation, reprinting, reuse of illustrations, recitation, broadcasting, reproduction on microfilms or in any other physical way, and transmission or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or hereafter developed. Exempted from this legal reservation are brief excerpts in connection with reviews or scholarly analysis or material supplied specifically for the purpose of being entered and executed on a computer system, for exclusive use by the purchaser of the work. Duplication of this publication or parts thereof is permitted only under the provisions of the Copyright Law of the Publisher's location, in its current version, and permission for use must always be obtained from Springer. Permissions for use may be obtained through RightsLink at the Copyright Clearance Center. Violations are liable to prosecution under the respective Copyright Law.  \nThe use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does not imply, even in the absence of a specific statement, that such names are exempt from the relevant protective laws and regulations and therefore free for general use.  \nWhile the advice and information in this book are believed to be true and accurate at the date of publication, neither the authors nor the editors nor the publisher can accept any legal responsibility for any errors or omissions that may be made. The publisher makes no warranty, express or implied, with respect to the material contained herein.  \nPrinted on acid-free paper  \nSpringer is part of Springer Science+Business Media (www.springer.com)  \n{2}------------------------------------------------",
      "metadata": {
        "header": "An Introduction to Statistical Learning",
        "subheader": "",
        "page": "0-2",
        "chunk_id": "chk_b640ceab-9484-413a-853e-6dbf8dcb58d4",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 370
      }
    },
    {
      "id": 463093387711036259,
      "distance": 1.2216053009033203,
      "text": "# Preface  \nStatistical learning refers to a set of tools for modeling and understanding complex datasets. It is a recently developed area in statistics and blends with parallel developments in computer science and, in particular, machine learning. The field encompasses many methods such as the lasso and sparse regression, classification and regression trees, and boosting and support vector machines.  \nWith the explosion of \"Big Data\" problems, statistical learning has become a very hot field in many scientific areas as well as marketing, finance, and other business disciplines. People with statistical learning skills are in high demand.  \nOne of the first books in this area—*The Elements of Statistical Learning* (ESL) (Hastie, Tibshirani, and Friedman)—was published in 2001, with a second edition in 2009. ESL has become a popular text not only in statistics but also in related fields. One of the reasons for ESL's popularity is its relatively accessible style. But ESL is intended for individuals with advanced training in the mathematical sciences. *An Introduction to Statistical Learning* (ISL) arose from the perceived need for a broader and less technical treatment of these topics. In this new book, we cover many of the same topics as ESL, but we concentrate more on the applications of the methods and less on the mathematical details. We have created labs illustrating how to implement each of the statistical learning methods using the popular statistical software package R. These labs provide the reader with valuable hands-on experience.  \nThis book is appropriate for advanced undergraduates or master's students in statistics or related quantitative fields or for individuals in other  \n{5}------------------------------------------------  \ndisciplines who wish to use statistical learning tools to analyze their data. It can be used as a textbook for a course spanning one or two semesters.  \nWe would like to thank several readers for valuable comments on preliminary drafts of this book: Pallavi Basu, Alexandra Chouldechova, Patrick Danaher, Will Fithian, Luella Fu, Sam Gross, Max Grazier G'Sell, Courtney Paulson, Xinghao Qiao, Elisa Sheng, Noah Simon, Kean Ming Tan, and Xin Lu Tan.  \n*It's tough to make predictions, especially about the future.*  \n-Yogi Berra  \nPalo Alto, USA Robert Tibshirani  \nLos Angeles, USA Gareth James Seattle, USA Daniela Witten Palo Alto, USA Trevor Hastie  \n{6}------------------------------------------------",
      "metadata": {
        "header": "Preface",
        "subheader": "",
        "page": "4-6",
        "chunk_id": "chk_391d3994-3d42-446b-ad81-d57eeeee3634",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 368
      }
    },
    {
      "id": 463093387711036273,
      "distance": 1.25235915184021,
      "text": "## Data Sets Used in Labs and Exercises  \nIn this textbook, we illustrate statistical learning methods using applications from marketing, finance, biology, and other areas. The ISLR package available on the book website contains a number of data sets that are required in order to perform the labs and exercises associated with this book. One other data set is contained in the MASS library, and yet another is part of the base R distribution. Table 1.1 contains a summary of the data sets required to perform the labs and exercises. A couple of these data sets are also available as text files on the book website, for use in Chapter 2.",
      "metadata": {
        "header": "Introduction",
        "subheader": "Data Sets Used in Labs and Exercises",
        "page": "24",
        "chunk_id": "chk_9264c39a-0c2e-422a-8827-25c51dc400ad",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 111
      }
    },
    {
      "id": 463093387711036316,
      "distance": 1.254095196723938,
      "text": "#### Select a Performance Measure  \nYour next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.  \nEquation 2-1. Root Mean Square Error (RMSE)  \nRMSE(\n$$\\mathbf{X}, h$$\n) =  $\\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} \\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2}$",
      "metadata": {
        "header": "<span id=\"page-34-0\"></span>**End-to-End Machine Learning Project**",
        "subheader": "Select a Performance Measure",
        "page": "39",
        "chunk_id": "chk_a5859879-bed6-4e1c-9e2e-849bbc3c3cfd",
        "file_id": "fi_ca503e7e-a03a-43a1-b13e-e6e666aa1e94",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\oreilly-p1\\oreilly-p1.md",
        "chunk_size": 77
      }
    },
    {
      "id": 463093387711036318,
      "distance": 1.254095196723938,
      "text": "#### Select a Performance Measure  \nYour next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It gives an idea of how much error the system typically makes in its predictions, with a higher weight for large errors. Equation 2-1 shows the mathematical formula to compute the RMSE.  \nEquation 2-1. Root Mean Square Error (RMSE)  \nRMSE(\n$$\\mathbf{X}, h$$\n) =  $\\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} \\left(h(\\mathbf{x}^{(i)}) - y^{(i)}\\right)^2}$  \n{1}------------------------------------------------",
      "metadata": {
        "header": "",
        "subheader": "Select a Performance Measure",
        "page": "0-1",
        "chunk_id": "chk_32e6416b-464e-4462-9317-ec4d55c2fa68",
        "file_id": "fi_05e25e12-d149-4c19-9dd8-33c3e88f82db",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\oreilly-p2\\oreilly-p2.md",
        "chunk_size": 78
      }
    },
    {
      "id": 463093387711036263,
      "distance": 1.2546637058258057,
      "text": "# Introduction  \n#### An Overview of Statistical Learning  \n*Statistical learning* refers to a vast set of tools for *understanding data*. These tools can be classified as *supervised* or *unsupervised*. Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an *output* based on one or more *inputs*. Problems of this nature occur in fields as diverse as business, medicine, astrophysics, and public policy. With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data. To provide an illustration of some applications of statistical learning, we briefly discuss three real-world data sets that are considered in this book.",
      "metadata": {
        "header": "Introduction",
        "subheader": "An Overview of Statistical Learning",
        "page": "12",
        "chunk_id": "chk_cf8f17b2-394d-4046-9e95-9b6a67462aeb",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 111
      }
    },
    {
      "id": 463093387711036268,
      "distance": 1.2714920043945312,
      "text": "#### A Brief History of Statistical Learning  \nThough the term *statistical learning* is fairly new, many of the concepts that underlie the field were developed long ago. At the beginning of the nineteenth century, Legendre and Gauss published papers on the *method*  \n{17}------------------------------------------------  \n*of least squares*, which implemented the earliest form of what is now known as *linear regression*. The approach was first successfully applied to problems in astronomy. Linear regression is used for predicting quantitative values, such as an individual's salary. In order to predict qualitative values, such as whether a patient survives or dies, or whether the stock market increases or decreases, Fisher proposed *linear discriminant analysis* in 1936. In the 1940s, various authors put forth an alternative approach, *logistic regression*. In the early 1970s, Nelder and Wedderburn coined the term *generalized linear models* for an entire class of statistical learning methods that include both linear and logistic regression as special cases.  \nBy the end of the 1970s, many more techniques for learning from data were available. However, they were almost exclusively *linear* methods, because fitting *non-linear* relationships was computationally infeasible at the time. By the 1980s, computing technology had finally improved sufficiently that non-linear methods were no longer computationally prohibitive. In mid 1980s Breiman, Friedman, Olshen and Stone introduced *classification and regression trees*, and were among the first to demonstrate the power of a detailed practical implementation of a method, including cross-validation for model selection. Hastie and Tibshirani coined the term *generalized additive models* in 1986 for a class of non-linear extensions to generalized linear models, and also provided a practical software implementation.  \nSince that time, inspired by the advent of *machine learning* and other disciplines, statistical learning has emerged as a new subfield in statistics, focused on supervised and unsupervised modeling and prediction. In recent years, progress in statistical learning has been marked by the increasing availability of powerful and relatively user-friendly software, such as the popular and freely available R system. This has the potential to continue the transformation of the field from a set of techniques used and developed by statisticians and computer scientists to an essential toolkit for a much broader community.",
      "metadata": {
        "header": "Introduction",
        "subheader": "A Brief History of Statistical Learning",
        "page": "16-17",
        "chunk_id": "chk_12f0c160-30da-44fd-b86e-f6b12e010e36",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 359
      }
    },
    {
      "id": 463093387711036267,
      "distance": 1.3305881023406982,
      "text": "#### Gene Expression Data  \nThe previous two applications illustrate data sets with both input and output variables. However, another important class of problems involves situations in which we only observe input variables, with no corresponding output. For example, in a marketing setting, we might have demographic information for a number of current or potential customers. We may wish to understand which types of customers are similar to each other by grouping individuals according to their observed characteristics. This is known as a *clustering* problem. Unlike in the previous examples, here we are not trying to predict an output variable.  \nWe devote Chapter 10 to a discussion of statistical learning methods for problems in which no natural output variable is available. We consider the NCI60 data set, which consists of 6,830 gene expression measurements for each of 64 cancer cell lines. Instead of predicting a particular output variable, we are interested in determining whether there are groups, or clusters, among the cell lines based on their gene expression measurements. This is a difficult question to address, in part because there are thousands of gene expression measurements per cell line, making it hard to visualize the data.  \nThe left-hand panel of Figure 1.4 addresses this problem by representing each of the 64 cell lines using just two numbers, Z<sup>1</sup> and Z2. These are the first two *principal components* of the data, which summarize the 6, 830 expression measurements for each cell line down to two numbers or *dimensions*. While it is likely that this dimension reduction has resulted in  \n{16}------------------------------------------------  \n![](_page_16_Figure_2.jpeg)  \n**FIGURE 1.4.** Left: Representation of the NCI60 gene expression data set in a two-dimensional space, Z<sup>1</sup> and Z2. Each point corresponds to one of the 64 cell lines. There appear to be four groups of cell lines, which we have represented using different colors. Right: Same as left panel except that we have represented each of the 14 different types of cancer using a different colored symbol. Cell lines corresponding to the same cancer type tend to be nearby in the two-dimensional space.  \nsome loss of information, it is now possible to visually examine the data for evidence of clustering. Deciding on the number of clusters is often a difficult problem. But the left-hand panel of Figure 1.4 suggests at least four groups of cell lines, which we have represented using separate colors. We can now examine the cell lines within each cluster for similarities in their types of cancer, in order to better understand the relationship between gene expression levels and cancer.  \nIn this particular data set, it turns out that the cell lines correspond to 14 different types of cancer. (However, this information was not used to create the left-hand panel of Figure 1.4.) The right-hand panel of Figure 1.4 is identical to the left-hand panel, except that the 14 cancer types are shown using distinct colored symbols. There is clear evidence that cell lines with the same cancer type tend to be located near each other in this two-dimensional representation. In addition, even though the cancer information was not used to produce the left-hand panel, the clustering obtained does bear some resemblance to some of the actual cancer types observed in the right-hand panel. This provides some independent verification of the accuracy of our clustering analysis.",
      "metadata": {
        "header": "Introduction",
        "subheader": "Gene Expression Data",
        "page": "15-16",
        "chunk_id": "chk_6d918c31-2dd5-48bf-9364-47e712286c5e",
        "file_id": "fi_cb52865a-6ff6-4200-85e0-67038ba6ffa3",
        "file_path": "C:\\Users\\Dell\\OneDrive - OnPoint Insights LLC\\Documents\\workspace\\blue_carbon\\utils\\data\\output\\ISLR_1stEd-1-30\\ISLR_1stEd-1-30.md",
        "chunk_size": 548
      }
    }
  ],
  "primary_chunks_count": 10,
  "total_chunks_retrieved": 10
}